# -*- coding: utf-8 -*-
"""Transform and Load.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QnHg_a3-cgGuPU75QVDA98WEP_cVsp_v


!pip install azure-storage-blob # Microoft Azure
!pip install pyarrow
!pip install psycopg2 sqlalchemy
!pip install pyodbc

import pandas as pd
import numpy as np
import json
import requests
from io import StringIO
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient
from math import ceil
import datetime
import calendar
from sqlalchemy import create_engine
import pyodbc

#Azure functions
def azure_upload_blob(connect_str, container_name, blob_name, data):
    blob_service_client = BlobServiceClient.from_connection_string(connect_str)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    blob_client.upload_blob(data, overwrite=True)
    print(f"Uploaded to Azure Blob: {blob_name}")

def azure_download_blob(connect_str, container_name, blob_name):
    blob_service_client = BlobServiceClient.from_connection_string(connect_str)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    download_stream = blob_client.download_blob()
    return download_stream.readall()

# Specify the path to your JSON configuration file
config_file_path = 'config.json'

# Load the JSON configuration file
with open(config_file_path, 'r') as config_file:
    config = json.load(config_file)

# Retrieve the connection string from the config
CONNECTION_STRING_AZURE_STORAGE = config["connectionString"]
CONTAINER_AZURE = "airqualityhw12"

# Initialize the BlobServiceClient
blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING_AZURE_STORAGE)

# Get the container client
container_client = blob_service_client.get_container_client(CONTAINER_AZURE)

# Function to read blob data and return DataFrame
def read_blob_as_dataframe(blob_name):
    blob_client = container_client.get_blob_client(blob=blob_name)
    blob_data = blob_client.download_blob()
    blob_content = blob_data.readall().decode('utf-8')
    return pd.read_csv(StringIO(blob_content))

# List all blobs in the specified container
blob_list = container_client.list_blobs()
for blob in blob_list:
    print("Blob name:", blob.name)
    # Read blob content into DataFrame
    df = read_blob_as_dataframe(blob.name)
    # Display the shape of the DataFrame
    print(df.shape)

    airquality_data = df.copy()

df.columns

df= df.drop(columns = "Message") #Message column is empty

df

"""# Creating the Date dimension"""

# Start and end date
start_date = df['Start_Date'].min()
end_date = df['Start_Date'].max()

print("Start Date:", start_date)
print("End Date:", end_date)

# Create Date Dimension

def week_of_month(dt):
    year = dt.year
    month = dt.month
    day = dt.day

    cal = calendar.monthcalendar(year, month)
    week_number = (day - 1) // 7 + 1
    return week_number

start_date = pd.to_datetime('2005-01-01')
end_date = pd.to_datetime('2022-06-01')
# Create a DataFrame for the date dimension
date_dimension = pd.DataFrame({'date': pd.date_range(start_date, end_date, freq='H')})

date_dimension.head(25)

# Extract attributes
date_dimension['Year_number'] = date_dimension['date'].dt.year
date_dimension['Quarter_number'] = date_dimension['date'].dt.quarter
date_dimension['Month_number'] = date_dimension['date'].dt.month
date_dimension['MonthName'] = date_dimension['date'].dt.strftime('%B')
date_dimension['Day_number'] = date_dimension['date'].dt.day
date_dimension['DayName'] = date_dimension['date'].dt.strftime('%A')
date_dimension['Hour_number'] = date_dimension['date'].dt.hour
date_dimension['Dateisoformat'] = date_dimension['date'].apply(lambda x: x.isoformat())
date_dimension['DateOriginalFormat'] = date_dimension['date']
date_dimension['Date_ID'] = date_dimension['date'].dt.strftime('%Y%m%d%H')

# Add week of the month and week of the year
date_dimension['WeekoftheMonth'] = date_dimension['date'].apply(week_of_month)
date_dimension['WeekofTheYear'] = date_dimension['date'].dt.strftime('%U')

new_order = ['Date_ID', 'DateOriginalFormat' , 'Dateisoformat','Year_number','Quarter_number','Month_number','Day_number','Hour_number','MonthName','DayName','WeekofTheYear','WeekoftheMonth']
date_dimension = date_dimension[new_order]

date_dimension

"""# Creating the Location Dimension"""

selected_join = df[["Geo Join ID","Geo Place Name"]]
selected_join

look_up = pd.read_csv("lookup_table.csv")
look_up

merged_table = pd.merge(selected_join, look_up, on='Geo Place Name', how='right')
merged_table = merged_table.drop_duplicates()
geography = merged_table.drop(columns = ["LookupID"])
geography

"""# Creating the indicator Dimension"""

#creating the indicator table
indicator = df[["Indicator ID","Unique ID","Measure","Measure Info"]]
indicator = indicator.rename(columns={"Indicator ID": "indicator_id", "Unique ID":"unique_ID","Measure": "measure", "Measure Info": "measure_info"})
indicator

"""# Creating the Fact Dimension

"""



selected_column = date_dimension["Date_ID"]
facts_airquality = df[["Unique ID", "Indicator ID", "Data Value","Geo Join ID"]]
facts_airquality["Date_ID"] = selected_column
facts_airquality





"""# Loading Data into Postgres"""

# Database connection URL
pwd = 'Cis9440!'
database_url = f'postgresql://linalan1231:{pwd}@airquality.postgres.database.azure.com/postgres'

# Create a SQLAlchemy engine
engine = create_engine(database_url)

geography.to_sql('geography', con=engine, if_exists='append', index=False)

indicator.to_sql('indicator', con=engine, if_exists='append', index=False)

date_dimension.to_sql('time', con=engine, if_exists='append', index=False)

facts_airquality.to_sql('facts_airquality', con=engine, if_exists='append', index=False)



